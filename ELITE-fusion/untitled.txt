我已确认您的代码（mp_trainer.py）已恢复原状。基于对原始代码的分析，我为您总结了离线评估效果差的根本原因：

现象回顾：
模型生成的路径极长（20-50跳），且包含大量无效循环。
跳数比（Hop Ratio）高达 6.0 以上（正常应接近 1.0-1.5）。
代码证据： 在 mp_trainer.py 的 _compute_path_reward 函数中（第 203 行）：
python
rewards.append(wp * PDR + wa * RAD + wb * RHC + wg * RRC)
PDR (数据包投递率)：通常接近 1.0（只要链路连通）。
RRC (剩余资源)：通常接近 1.0（资源充足时）。
RAD (时延惩罚)：虽然是负值，但通常较小（例如 -0.1）。
总和：0.5*1.0 + 0.25*(-0.1) + 0.25*1.0 ≈ +0.725。
核心逻辑漏洞：
正向循环 (Positive Loop)：智能体每多走一步，就能获得约 +0.7 的正奖励。
缺乏约束：代码中没有设置“单步惩罚”（例如 -1.0）来抵消这个正奖励。
缺乏诱导：代码中没有设置“终点大奖”（例如 +10.0）来吸引智能体尽快到达终点。
结论： 智能体并没有“学坏”，它只是太“聪明”了——它发现赖在网络里兜圈子比直接去终点得分更高。这是一个典型的强化学习 Reward Hacking 问题。